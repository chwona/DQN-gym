{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1jroj6k1thaSCOIiEuWYRvjFFSmGy6TK-","authorship_tag":"ABX9TyNDy+U8gyqEq5vALAuU5e/F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"OoaF-jA-5mmS","executionInfo":{"status":"ok","timestamp":1702560230058,"user_tz":-540,"elapsed":541,"user":{"displayName":"chaewon lee","userId":"17505574032484187684"}}},"outputs":[],"source":["################################################################################\n","# Program : Train_Breakout_DQN.py\n","# Description : Open AI GYM의 Taxi 환경의 강화학습 구현 코드 (코랩용) : DQN Breakout\n","################################################################################"]},{"cell_type":"code","source":["##################################################\n","# install package\n","##################################################\n","!pip install -U gym>=0.21.0\n","!pip install -U gym[atari,accept-rom-license]"],"metadata":{"id":"qoSZgVxFBGpy","executionInfo":{"status":"ok","timestamp":1702560267767,"user_tz":-540,"elapsed":37245,"user":{"displayName":"chaewon lee","userId":"17505574032484187684"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b9b4d131-76b4-44f1-fb61-2b658b2b2972"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","dopamine-rl 4.0.6 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mRequirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.26.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n","Collecting autorom[accept-rom-license]~=0.4.2 (from gym[accept-rom-license,atari])\n","  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Collecting ale-py~=0.8.0 (from gym[accept-rom-license,atari])\n","  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (6.1.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (4.5.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.66.1)\n","Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari])\n","  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2023.11.17)\n","Building wheels for collected packages: AutoROM.accept-rom-license\n","  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=bc8710ccf50a170d1a5daed18afb8a254700eebfd7271ce6e6edfd12ed895afd\n","  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n","Successfully built AutoROM.accept-rom-license\n","Installing collected packages: ale-py, AutoROM.accept-rom-license, autorom\n","Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2\n"]}]},{"cell_type":"code","source":["##################################################\n","# import package\n","##################################################\n","import gym\n","import numpy as np\n","from skimage import transform, color\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n","from tensorflow.keras.optimizers import Adam\n","\n","from collections import deque\n","import random\n","\n","from joblib import dump, load"],"metadata":{"id":"hq5PIpZOBHQr","executionInfo":{"status":"ok","timestamp":1702560284292,"user_tz":-540,"elapsed":5964,"user":{"displayName":"chaewon lee","userId":"17505574032484187684"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["##################################################\n","# Define Class and Function\n","##################################################\n","#전처리용\n","def preprocess_frame(frame):\n","    cropped_frame = frame[35:195, 8:152]\n","    grayscale_frame = color.rgb2gray(cropped_frame)\n","    normalized_frame = grayscale_frame / 255.0\n","    preprocessed_frame = transform.resize(normalized_frame, (84, 84))\n","    preprocessed_frame = preprocessed_frame.astype(np.uint8)\n","\n","    return preprocessed_frame\n","\n","#엡실론그리디\n","class EpsilonGreedy:\n","    def __init__(self, max_epsilon, min_epsilon, dacay_rate):\n","        self.max_epsilon = max_epsilon\n","        self.min_epsilon = min_epsilon\n","        self.dacay_rate = dacay_rate\n","        self.epsilon = max_epsilon\n","\n","    def exploration_rate(self, step):\n","        self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * np.exp(-self.dacay_rate * step)\n","        return self.epsilon\n","\n","# DNN 모델 생성\n","def build_model(input_shape, num_actions):\n","    model = Sequential()\n","    model.add(Convolution2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=input_shape))\n","    model.add(Convolution2D(64, (4, 4), strides=(2, 2), activation='relu'))\n","    model.add(Convolution2D(64, (3, 3), activation='relu'))\n","    model.add(Flatten())\n","    model.add(Dense(512, activation='relu'))\n","    model.add(Dense(num_actions, activation='linear'))\n","    model.compile(loss='mse', optimizer=Adam(lr=0.0001))\n","    return model\n","\n","# Experience Replay Buffer\n","class ReplayBuffer:\n","    def __init__(self, max_size):\n","        self.buffer = deque(maxlen=max_size)\n","\n","    def add_exp(self, stacked_frame, action, reward, next_stacked_frame, done):\n","        self.buffer.append((stacked_frame, action, reward, next_stacked_frame, done))\n","\n","    def sample_mbatch(self, batch_size):\n","        batch = random.sample(self.buffer, batch_size)\n","        stacked_frames, actions, rewards, next_stacked_frames, dones = zip(*batch)\n","        return np.array(stacked_frames), np.array(actions), np.array(rewards, dtype=np.float32), np.array(next_stacked_frames), np.array(dones, dtype=np.uint8)\n","        # return np.stack(stacked_frames), np.stack(actions), np.stack(rewards, dtype=np.float32), np.stack(next_stacked_frames), np.stack(dones, dtype=np.uint8)"],"metadata":{"id":"EnFyoJsnBLRY","executionInfo":{"status":"ok","timestamp":1702560284293,"user_tz":-540,"elapsed":4,"user":{"displayName":"chaewon lee","userId":"17505574032484187684"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["##################################################\n","# Hyperparameters\n","##################################################\n","max_mermory_size = 100000000\n","batch_size = 32\n","gamma = 0.99  # Discount factor\n","max_epsilon = 1.0\n","min_epsilon = 0.01\n","dacay_rate = 0.001\n","#dacay_rate = 0.0001\n","update_period = 10 #에피소드 기준"],"metadata":{"id":"Yv7K62ikBLIx","executionInfo":{"status":"ok","timestamp":1702560284293,"user_tz":-540,"elapsed":3,"user":{"displayName":"chaewon lee","userId":"17505574032484187684"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["##################################################\n","# Set Envrionment\n","##################################################\n","env = gym.make('Breakout-v0')\n","# NameNotFound: Environment Breakout doesn't exist.\n","# 해당 에러가 발생할 경우\n","# Taxi_Train/Test_DQN 코드 실행 이후, 해당 코드를 실행시키면 발생합니다.\n","# Taxi_Train/Test_DQN과 해당 코드에서 사용하는 Gym 버전이 달라서 발생하는 것으로 추정합니다.\n","# 이 경우 런타임 해제 후 맨 상단의 pip gym 코드를 다시 실행시킵니다.\n","\n","num_actions = env.action_space.n\n","input_shape = env.observation_space.shape\n","print('state 구조 : {}'.format(input_shape))\n","print('action 개수 : {}'.format(num_actions))\n","\n","# 전처리한 버전으로 shape 재 설정\n","input_shape = (84, 84, 4)\n","\n","# 모델 생성\n","dqn = build_model(input_shape, num_actions)\n","target_dqn = build_model(input_shape, num_actions)\n","target_dqn.set_weights(dqn.get_weights())\n","\n","# Replay Buffer 설정\n","replay_buffer = ReplayBuffer(max_mermory_size)\n","\n","# 엡실론그리디\n","exploration_strategy = EpsilonGreedy(max_epsilon, min_epsilon, dacay_rate)\n"],"metadata":{"id":"wNCb9Jq1BK_A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702560454269,"user_tz":-540,"elapsed":1038,"user":{"displayName":"chaewon lee","userId":"17505574032484187684"}},"outputId":"2c289914-3011-4e9f-d78d-01a6c572eb2f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"output_type":"stream","name":"stdout","text":["state 구조 : (210, 160, 3)\n","action 개수 : 4\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]}]},{"cell_type":"code","source":["##################################################\n","# Train\n","##################################################\n","train_episodes = []\n","episode_rewards = [] # train_total_rewards\n","train_total_steps = []\n","train_epsilon = []\n","\n","num_episodes = 5000\n","no_op_steps = 30\n","\n","for episode in range(num_episodes):\n","\n","    state, _ = env.reset()\n","    # 초반 30 프레임 제외\n","    for _ in range(random.randint(1, no_op_steps)):\n","        state, _, _, _, _ = env.step(1)\n","\n","    # 프레임을 전처리 이후 가장 첫번째 stacked_frame은 같은 state 4개 stack해서 사용\n","    state = preprocess_frame(state)\n","    stacked_frame = np.stack([state] * 4, axis=2)\n","\n","    # reset\n","    episode_reward = 0\n","    total_step = 0\n","    done = False\n","\n","    # action 0 (NOOP) -> Fire 대체하여 게임 재시작하도록 유도\n","    action_set = {0:1, 1:2, 2:3, 3:3}\n","\n","    while not done:\n","        # rate = exploration_strategy.exploration_rate(total_step)\n","        rate = exploration_strategy.exploration_rate(episode)\n","        if np.random.rand() < rate:\n","            action = env.action_space.sample()\n","        else:\n","            # q_values = dqn.predict(np.expand_dims(state, axis=0))\n","            q_values = dqn.predict(np.expand_dims(stacked_frame, axis=0), verbose=0) #stacked_frames\n","            action = np.argmax(q_values)\n","\n","        # action 0 (NOOP) -> Fire 대체하여 게임 재시작하도록 유도\n","        action_2 = action_set[action]\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action_2)\n","        # 다음 상태 전처리\n","        next_state = preprocess_frame(next_state)\n","        # stacked_frame에 새로 쌓기\n","        next_stacked_frame = np.append(stacked_frame[:, :, 1:], np.expand_dims(next_state, axis=2), axis=2)\n","\n","        done = (terminated or truncated)\n","        episode_reward += reward\n","\n","        # 경험 저장\n","        replay_buffer.add_exp(stacked_frame, action, reward, next_stacked_frame, done)\n","\n","        stacked_frame = next_stacked_frame\n","        total_step += 1\n","\n","        if done:\n","            break\n","\n","    # Mini-Batch 학습\n","    if len(replay_buffer.buffer) > 3000: # 경험이 3000개 초과 저장되면\n","    # if len(replay_buffer.buffer) > batch_size:\n","\n","        # 32 * 10 = 320개 Row 학습\n","        for i in range(10):\n","\n","            stacked_frames, actions, rewards, next_stacked_frames, dones = replay_buffer.sample_mbatch(batch_size)\n","\n","            # 일반 네트워크에서의 예측값\n","            targets = dqn.predict(stacked_frames, verbose=0)\n","\n","            # s'로 인한 타겟 네트워크에서의 예측값\n","            next_q = target_dqn.predict(next_stacked_frames, verbose=0)\n","\n","            # 벨먼 방정식 적용 후 학습\n","            targets[np.arange(batch_size), actions] = rewards + gamma * np.max(next_q, axis=1) * (1 - dones)\n","            dqn.train_on_batch(stacked_frames, targets)\n","\n","\t\t# 주기별 일반네트워크의 가중치 타겟 네트워크로 적용\n","        if episode % update_period == 0 and episode != 0:\n","            target_dqn.set_weights(dqn.get_weights())\n","\n","    train_episodes.append(episode)\n","    episode_rewards.append(episode_reward)\n","    train_total_steps.append(total_step)\n","    train_epsilon.append(rate)\n","\n","    # 주기별 모델 저장\n","    if (episode + 1) % 100 == 0:\n","        target_dqn.save(f'/content/drive/MyDrive/RL/Breakout/breakout_model_e{episode}.h5')\n","\n","    # 중간에 끊겼을 때 저장용\n","    dump(episode, '/content/drive/MyDrive/RL/Breakout/breakout_latest_episode.joblib')\n","    dump(episode_reward, '/content/drive/MyDrive/RL/Breakout/breakout_latest_episode_reward.joblib')\n","    dump(rate, '/content/drive/MyDrive/RL/Breakout/breakout_latest_exploration_rate.joblib')\n","\n","    print(f\"Episode: {episode + 1}/{num_episodes}, Total Reward: {episode_reward}, Epsilon: {rate}, Total step: {total_step}\")\n","\n","#마지막 에피소드의 모델 저장\n","target_dqn.save(f'/content/drive/MyDrive/RL/Breakout/breakout_model_e{episode}.h5')\n","dump(train_episodes, '/content/drive/MyDrive/RL/Breakout/breakout_train_episodes.joblib')\n","dump(episode_rewards, '/content/drive/MyDrive/RL/Breakout/breakout_episode_rewards.joblib')\n","dump(train_total_steps, '/content/drive/MyDrive/RL/Breakout/breakout_train_total_steps.joblib')\n","dump(train_epsilon, '/content/drive/MyDrive/RL/Breakout/breakout_train_epsilon.joblib')\n","\n","print(\"Train Complete!\")"],"metadata":{"id":"ixees7akBKtB","colab":{"base_uri":"https://localhost:8080/","height":568},"executionInfo":{"status":"error","timestamp":1702560484465,"user_tz":-540,"elapsed":7676,"user":{"displayName":"chaewon lee","userId":"17505574032484187684"}},"outputId":"5c62f0cc-4e76-47b4-e0ba-86c04ffdb410"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 1/5000, Total Reward: 0.0, Epsilon: 1.0, Total step: 144\n","Episode: 2/5000, Total Reward: 1.0, Epsilon: 0.9990104948350412, Total step: 197\n","Episode: 3/5000, Total Reward: 1.0, Epsilon: 0.9980219786806598, Total step: 188\n","Episode: 4/5000, Total Reward: 0.0, Epsilon: 0.9970344505483393, Total step: 160\n","Episode: 5/5000, Total Reward: 1.0, Epsilon: 0.9960479094505515, Total step: 192\n","Episode: 6/5000, Total Reward: 0.0, Epsilon: 0.9950623544007555, Total step: 149\n","Episode: 7/5000, Total Reward: 0.0, Epsilon: 0.9940777844133959, Total step: 166\n","Episode: 8/5000, Total Reward: 2.0, Epsilon: 0.9930941985039028, Total step: 273\n","Episode: 9/5000, Total Reward: 5.0, Epsilon: 0.99211159568869, Total step: 420\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-f9cfe30f4c4b>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0maction_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m# 다음 상태 전처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ale_py/env/gym.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action_ind)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0mis_terminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_truncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mis_truncated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}